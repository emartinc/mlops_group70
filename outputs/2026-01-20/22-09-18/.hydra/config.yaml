data:
  _target_: mbti_classifier.data.MBTIDataModule
  raw_data_path: data/raw
  processed_data_path: data/processed
  batch_size: 16
  num_workers: 4
  test_size: 0.15
  val_size: 0.15
  random_seed: ${seed}
  model_name: distilbert-base-uncased
  max_length: 512
  cache_dir: null
model:
  _target_: mbti_classifier.model.MBTIClassifier
  model_name: distilbert-base-uncased
  learning_rate: 2.0e-05
  weight_decay: 0.01
  warmup_steps: 500
  dropout: 0.1
  freeze_encoder: false
  freeze_layers: 0
trainer:
  _target_: pytorch_lightning.Trainer
  max_epochs: 10
  accelerator: auto
  devices: auto
  precision: 32
  gradient_clip_val: 1.0
  accumulate_grad_batches: 1
  deterministic: false
  log_every_n_steps: 10
  val_check_interval: 1.0
  enable_checkpointing: true
  enable_progress_bar: true
  enable_model_summary: true
callbacks:
  model_checkpoint:
    _target_: pytorch_lightning.callbacks.ModelCheckpoint
    dirpath: models
    filename: best
    monitor: val/avg_f1
    mode: max
    save_top_k: 1
    save_last: false
    verbose: true
    auto_insert_metric_name: false
  lr_monitor:
    _target_: pytorch_lightning.callbacks.LearningRateMonitor
    logging_interval: step
  progress_bar:
    _target_: pytorch_lightning.callbacks.RichProgressBar
  early_stopping:
    _target_: pytorch_lightning.callbacks.EarlyStopping
    monitor: val/avg_f1
    patience: 3
    mode: max
    verbose: true
    min_delta: 0.001
logger:
  _target_: pytorch_lightning.loggers.WandbLogger
  project: mbti-classification
  save_dir: logs
  log_model: false
  tags:
  - distilbert
  - mbti
  - fine-tuning
  name: ${experiment_name}
experiment_name: mbti_distilbert
seed: 42
early_stopping_enabled: true
run_test: true
use_wandb: true
