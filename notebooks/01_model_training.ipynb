{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993909ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "988fef17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prg/GitHub/mlops_group70/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "import mlcroissant as mlc\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class MBTIDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for MBTI personality types.\"\"\"\n",
    "    \n",
    "    def __init__(self, texts, labels):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            texts: List or array of text posts\n",
    "            labels: Tensor of integer labels (0-15)\n",
    "        \"\"\"\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'text': self.texts[idx],\n",
    "            'label': self.labels[idx]\n",
    "        }\n",
    "\n",
    "\n",
    "class MBTIDataModule(pl.LightningDataModule):\n",
    "    \"\"\"\n",
    "    PyTorch Lightning DataModule for MBTI personality classification.\n",
    "    \n",
    "    Handles data downloading, cleaning, preprocessing, and DataLoader creation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        raw_data_path: str = \"data/raw\",\n",
    "        processed_data_path: str = \"data/processed\",\n",
    "        batch_size: int = 32,\n",
    "        num_workers: int = 4,\n",
    "        test_size: float = 0.2,\n",
    "        val_size: float = 0.1,\n",
    "        random_seed: int = 42\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            raw_data_path: Path to store raw downloaded data\n",
    "            processed_data_path: Path to store processed data\n",
    "            batch_size: Batch size for DataLoaders\n",
    "            num_workers: Number of workers for DataLoaders\n",
    "            test_size: Proportion of data for test set\n",
    "            val_size: Proportion of train data for validation set\n",
    "            random_seed: Random seed for reproducibility\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.raw_data_path = Path(raw_data_path)\n",
    "        self.processed_data_path = Path(processed_data_path)\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.test_size = test_size\n",
    "        self.val_size = val_size\n",
    "        self.random_seed = random_seed\n",
    "        \n",
    "        # Will be populated during setup\n",
    "        self.train_dataset = None\n",
    "        self.val_dataset = None\n",
    "        self.test_dataset = None\n",
    "        self.type_to_idx = None\n",
    "        self.idx_to_type = None\n",
    "        self.num_classes = None\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        \"\"\"\n",
    "        Download data if needed. Called only on 1 GPU in distributed settings.\n",
    "        \"\"\"\n",
    "        self._ensure_data()\n",
    "    \n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Load and split data. Called on every GPU in distributed settings.\n",
    "        \n",
    "        Args:\n",
    "            stage: 'fit', 'validate', 'test', or 'predict'\n",
    "        \"\"\"\n",
    "        # Load and process data\n",
    "        df = self._load_and_clean_data()\n",
    "        \n",
    "        # Create label mappings\n",
    "        unique_types = sorted(df['type'].unique())\n",
    "        self.type_to_idx = {t: i for i, t in enumerate(unique_types)}\n",
    "        self.idx_to_type = {i: t for t, i in self.type_to_idx.items()}\n",
    "        self.num_classes = len(unique_types)\n",
    "        \n",
    "        df['type_idx'] = df['type'].map(self.type_to_idx)\n",
    "        \n",
    "        # Split into train+val and test\n",
    "        train_val_df, test_df = train_test_split(\n",
    "            df,\n",
    "            test_size=self.test_size,\n",
    "            random_state=self.random_seed,\n",
    "            stratify=df['type']\n",
    "        )\n",
    "        \n",
    "        # Split train into train and validation\n",
    "        train_df, val_df = train_test_split(\n",
    "            train_val_df,\n",
    "            test_size=self.val_size,\n",
    "            random_state=self.random_seed,\n",
    "            stratify=train_val_df['type']\n",
    "        )\n",
    "        \n",
    "        # Create datasets\n",
    "        if stage == 'fit' or stage is None:\n",
    "            self.train_dataset = MBTIDataset(\n",
    "                train_df['posts'].values,\n",
    "                torch.tensor(train_df['type_idx'].values, dtype=torch.long)\n",
    "            )\n",
    "            self.val_dataset = MBTIDataset(\n",
    "                val_df['posts'].values,\n",
    "                torch.tensor(val_df['type_idx'].values, dtype=torch.long)\n",
    "            )\n",
    "        \n",
    "        if stage == 'test' or stage is None:\n",
    "            self.test_dataset = MBTIDataset(\n",
    "                test_df['posts'].values,\n",
    "                torch.tensor(test_df['type_idx'].values, dtype=torch.long)\n",
    "            )\n",
    "        \n",
    "        logger.info(f\"Data split: Train={len(train_df)}, Val={len(val_df)}, Test={len(test_df)}\")\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        \"\"\"Create training DataLoader.\"\"\"\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        \"\"\"Create validation DataLoader.\"\"\"\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True\n",
    "        )\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        \"\"\"Create test DataLoader.\"\"\"\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True\n",
    "        )\n",
    "    \n",
    "    def _ensure_data(self):\n",
    "        \"\"\"Download dataset if not present locally.\"\"\"\n",
    "        csv_path = self.raw_data_path / \"mbti_1.csv\"\n",
    "        \n",
    "        if csv_path.exists():\n",
    "            logger.info(f\"Raw data found at: {csv_path}\")\n",
    "            return csv_path\n",
    "        \n",
    "        logger.info(\"Downloading dataset via mlcroissant...\")\n",
    "        self.raw_data_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        try:\n",
    "            url = \"https://www.kaggle.com/datasets/datasnaek/mbti-type/croissant/download\"\n",
    "            dataset = mlc.Dataset(url)\n",
    "            record_sets = dataset.metadata.record_sets\n",
    "            records = dataset.records(record_set=record_sets[0].uuid)\n",
    "            df = pd.DataFrame(records)\n",
    "            \n",
    "            # Clean column names\n",
    "            df.columns = [col.split(\"/\")[-1] for col in df.columns]\n",
    "            \n",
    "            df.to_csv(csv_path, index=False)\n",
    "            logger.info(f\"Dataset saved to: {csv_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error downloading dataset: {e}\")\n",
    "            raise\n",
    "        \n",
    "        return csv_path\n",
    "    \n",
    "    def _load_and_clean_data(self):\n",
    "        \"\"\"Load and clean the MBTI dataset.\"\"\"\n",
    "        csv_path = self.raw_data_path / \"mbti_1.csv\"\n",
    "        df = pd.read_csv(csv_path)\n",
    "        logger.info(f\"Loaded {len(df)} rows\")\n",
    "        \n",
    "        # Clean 'type' column (remove byte string artifacts)\n",
    "        if df['type'].dtype == object:\n",
    "            df['type'] = df['type'].astype(str).str.replace(r\"^b'|'$\", \"\", regex=True)\n",
    "        \n",
    "        # Clean 'posts' column\n",
    "        df['posts'] = df['posts'].astype(str).apply(self._clean_text)\n",
    "        \n",
    "        # Add binary features (optional, can be used for multi-task learning)\n",
    "        df['is_E'] = df['type'].apply(lambda x: 1 if 'E' in x else 0)\n",
    "        df['is_S'] = df['type'].apply(lambda x: 1 if 'S' in x else 0)\n",
    "        df['is_T'] = df['type'].apply(lambda x: 1 if 'T' in x else 0)\n",
    "        df['is_J'] = df['type'].apply(lambda x: 1 if 'J' in x else 0)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def _clean_text(text: str) -> str:\n",
    "        \"\"\"\n",
    "        Clean text data by removing URLs, byte artifacts, and normalizing whitespace.\n",
    "        \n",
    "        Args:\n",
    "            text: Raw text string\n",
    "            \n",
    "        Returns:\n",
    "            Cleaned text string\n",
    "        \"\"\"\n",
    "        # Remove byte string prefixes\n",
    "        if text.startswith(\"b'\") or text.startswith('b\"'):\n",
    "            text = text[2:-1]\n",
    "        \n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
    "        \n",
    "        # Remove pipe separators and normalize\n",
    "        text = text.replace('|||', ' ')\n",
    "        text = text.lower()\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        return text\n",
    "\n",
    "\n",
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Initialize DataModule\n",
    "#     dm = MBTIDataModule(\n",
    "#         batch_size=32,\n",
    "#         num_workers=4\n",
    "#     )\n",
    "    \n",
    "#     # Prepare and setup data\n",
    "#     dm.prepare_data()\n",
    "#     dm.setup()\n",
    "    \n",
    "#     # Access dataloaders\n",
    "#     train_loader = dm.train_dataloader()\n",
    "#     val_loader = dm.val_dataloader()\n",
    "#     test_loader = dm.test_dataloader()\n",
    "    \n",
    "#     # Print information\n",
    "#     print(f\"Number of classes: {dm.num_classes}\")\n",
    "#     print(f\"Type to index mapping: {dm.type_to_idx}\")\n",
    "#     print(f\"Train batches: {len(train_loader)}\")\n",
    "#     print(f\"Val batches: {len(val_loader)}\")\n",
    "#     print(f\"Test batches: {len(test_loader)}\")\n",
    "    \n",
    "#     # Example: Iterate through one batch\n",
    "#     for batch in train_loader:\n",
    "#         print(f\"\\nBatch text sample: {batch['text'][0][:100]}...\")\n",
    "#         print(f\"Batch labels shape: {batch['label'].shape}\")\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e55e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Optional, Dict, Any\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from transformers import (\n",
    "    DistilBertTokenizer,\n",
    "    DistilBertForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from torchmetrics import Accuracy, Precision, Recall, F1Score, ConfusionMatrix\n",
    "# Import metrics collection from torchmetrics\n",
    "from torchmetrics import MetricCollection\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class MBTIClassifier(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    DistilBERT-based classifier for MBTI personality type prediction.\n",
    "    Supports fine-tuning with comprehensive WandB logging.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int = 16,\n",
    "        model_name: str = \"distilbert-base-uncased\",\n",
    "        learning_rate: float = 2e-5,\n",
    "        weight_decay: float = 0.01,\n",
    "        warmup_steps: int = 500,\n",
    "        max_length: int = 512,\n",
    "        dropout_rate: float = 0.1,\n",
    "        freeze_encoder: bool = False,\n",
    "        freeze_layers: int = 0\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_classes: Number of personality types (16 for MBTI)\n",
    "            model_name: HuggingFace model identifier\n",
    "            learning_rate: Learning rate for optimizer\n",
    "            weight_decay: Weight decay for AdamW\n",
    "            warmup_steps: Number of warmup steps for scheduler\n",
    "            max_length: Maximum sequence length for tokenizer\n",
    "            dropout_rate: Dropout rate for classification head\n",
    "            freeze_encoder: Whether to freeze entire encoder\n",
    "            freeze_layers: Number of encoder layers to freeze (0 = none)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # Load tokenizer and model\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "        self.model = DistilBertForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=num_classes,\n",
    "            dropout=dropout_rate\n",
    "        )\n",
    "        \n",
    "        # Freeze layers if specified\n",
    "        if freeze_encoder:\n",
    "            for param in self.model.distilbert.parameters():\n",
    "                param.requires_grad = False\n",
    "        elif freeze_layers > 0:\n",
    "            for layer in self.model.distilbert.transformer.layer[:freeze_layers]:\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = False\n",
    "        \n",
    "        # Loss function\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Metrics for each split\n",
    "        self.training_metrics = MetricCollection({\n",
    "            'acc': Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.train_acc = Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.val_acc = Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.test_acc = Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        \n",
    "        self.val_precision = Precision(task=\"multiclass\", num_classes=num_classes, average='macro')\n",
    "        self.val_recall = Recall(task=\"multiclass\", num_classes=num_classes, average='macro')\n",
    "        self.val_f1 = F1Score(task=\"multiclass\", num_classes=num_classes, average='macro')\n",
    "        \n",
    "        self.test_precision = Precision(task=\"multiclass\", num_classes=num_classes, average='macro')\n",
    "        self.test_recall = Recall(task=\"multiclass\", num_classes=num_classes, average='macro')\n",
    "        self.test_f1 = F1Score(task=\"multiclass\", num_classes=num_classes, average='macro')\n",
    "        \n",
    "        # Confusion matrix for test\n",
    "        self.test_confusion = ConfusionMatrix(task=\"multiclass\", num_classes=num_classes)\n",
    "        \n",
    "        # Store predictions for analysis\n",
    "        self.validation_step_outputs = []\n",
    "        self.test_step_outputs = []\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"Forward pass through the model.\"\"\"\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return outputs.logits\n",
    "    \n",
    "    def _shared_step(self, batch, batch_idx):\n",
    "        \"\"\"Common step for train/val/test.\"\"\"\n",
    "        texts = batch['text']\n",
    "        labels = batch['label']\n",
    "        \n",
    "        # Tokenize\n",
    "        encoding = self.tokenizer(\n",
    "            texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.hparams.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        input_ids = encoding['input_ids'].to(self.device)\n",
    "        attention_mask = encoding['attention_mask'].to(self.device)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = self(input_ids, attention_mask)\n",
    "        loss = self.criterion(logits, labels)\n",
    "        \n",
    "        # Predictions\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        \n",
    "        return loss, preds, labels, logits\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"Training step.\"\"\"\n",
    "        loss, preds, labels, _ = self._shared_step(batch, batch_idx)\n",
    "        \n",
    "        # Update metrics\n",
    "        self.train_acc(preds, labels)\n",
    "        \n",
    "        # Log metrics\n",
    "        self.log('train/loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('train/acc', self.train_acc, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \"\"\"Validation step.\"\"\"\n",
    "        loss, preds, labels, logits = self._shared_step(batch, batch_idx)\n",
    "        \n",
    "        # Update metrics\n",
    "        self.val_acc(preds, labels)\n",
    "        self.val_precision(preds, labels)\n",
    "        self.val_recall(preds, labels)\n",
    "        self.val_f1(preds, labels)\n",
    "        \n",
    "        # Log metrics\n",
    "        self.log('val/loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log('val/acc', self.val_acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log('val/precision', self.val_precision, on_step=False, on_epoch=True)\n",
    "        self.log('val/recall', self.val_recall, on_step=False, on_epoch=True)\n",
    "        self.log('val/f1', self.val_f1, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        \n",
    "        # Store outputs for epoch-level analysis\n",
    "        self.validation_step_outputs.append({\n",
    "            'preds': preds,\n",
    "            'labels': labels,\n",
    "            'loss': loss\n",
    "        })\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        \"\"\"Log additional validation metrics at epoch end.\"\"\"\n",
    "        if len(self.validation_step_outputs) == 0:\n",
    "            return\n",
    "        \n",
    "        # Calculate per-class accuracy\n",
    "        all_preds = torch.cat([x['preds'] for x in self.validation_step_outputs])\n",
    "        all_labels = torch.cat([x['labels'] for x in self.validation_step_outputs])\n",
    "        \n",
    "        # Log per-class metrics if using WandB\n",
    "        if isinstance(self.logger, WandbLogger):\n",
    "            per_class_acc = {}\n",
    "            for class_idx in range(self.hparams.num_classes):\n",
    "                mask = all_labels == class_idx\n",
    "                if mask.sum() > 0:\n",
    "                    class_acc = (all_preds[mask] == all_labels[mask]).float().mean()\n",
    "                    per_class_acc[f'val/class_{class_idx}_acc'] = class_acc.item()\n",
    "            \n",
    "            self.logger.experiment.log(per_class_acc)\n",
    "        \n",
    "        self.validation_step_outputs.clear()\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        \"\"\"Test step.\"\"\"\n",
    "        loss, preds, labels, logits = self._shared_step(batch, batch_idx)\n",
    "        \n",
    "        # Update metrics\n",
    "        self.test_acc(preds, labels)\n",
    "        self.test_precision(preds, labels)\n",
    "        self.test_recall(preds, labels)\n",
    "        self.test_f1(preds, labels)\n",
    "        self.test_confusion(preds, labels)\n",
    "        \n",
    "        # Log metrics\n",
    "        self.log('test/loss', loss, on_step=False, on_epoch=True)\n",
    "        self.log('test/acc', self.test_acc, on_step=False, on_epoch=True)\n",
    "        self.log('test/precision', self.test_precision, on_step=False, on_epoch=True)\n",
    "        self.log('test/recall', self.test_recall, on_step=False, on_epoch=True)\n",
    "        self.log('test/f1', self.test_f1, on_step=False, on_epoch=True)\n",
    "        \n",
    "        # Store outputs for final analysis\n",
    "        self.test_step_outputs.append({\n",
    "            'preds': preds,\n",
    "            'labels': labels,\n",
    "            'logits': logits\n",
    "        })\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def on_test_epoch_end(self):\n",
    "        \"\"\"Log comprehensive test results.\"\"\"\n",
    "        if len(self.test_step_outputs) == 0:\n",
    "            return\n",
    "        \n",
    "        all_preds = torch.cat([x['preds'] for x in self.test_step_outputs])\n",
    "        all_labels = torch.cat([x['labels'] for x in self.test_step_outputs])\n",
    "        \n",
    "        # Compute confusion matrix\n",
    "        cm = self.test_confusion.compute()\n",
    "        \n",
    "        # Log confusion matrix to WandB\n",
    "        if isinstance(self.logger, WandbLogger):\n",
    "            # Per-class metrics\n",
    "            per_class_metrics = {}\n",
    "            for class_idx in range(self.hparams.num_classes):\n",
    "                mask = all_labels == class_idx\n",
    "                if mask.sum() > 0:\n",
    "                    class_acc = (all_preds[mask] == all_labels[mask]).float().mean()\n",
    "                    per_class_metrics[f'test/class_{class_idx}_acc'] = class_acc.item()\n",
    "            \n",
    "            self.logger.experiment.log(per_class_metrics)\n",
    "            \n",
    "            # Log confusion matrix as heatmap\n",
    "            self.logger.experiment.log({\n",
    "                \"test/confusion_matrix\": wandb.plot.confusion_matrix(\n",
    "                    probs=None,\n",
    "                    y_true=all_labels.cpu().numpy(),\n",
    "                    preds=all_preds.cpu().numpy(),\n",
    "                    class_names=[str(i) for i in range(self.hparams.num_classes)]\n",
    "                )\n",
    "            })\n",
    "        \n",
    "        self.test_step_outputs.clear()\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Configure optimizer and learning rate scheduler.\"\"\"\n",
    "        # Separate parameters for different learning rates (optional)\n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                'params': [p for n, p in self.model.named_parameters() \n",
    "                          if not any(nd in n for nd in no_decay) and p.requires_grad],\n",
    "                'weight_decay': self.hparams.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                'params': [p for n, p in self.model.named_parameters() \n",
    "                          if any(nd in n for nd in no_decay) and p.requires_grad],\n",
    "                'weight_decay': 0.0,\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        optimizer = torch.optim.AdamW(\n",
    "            optimizer_grouped_parameters,\n",
    "            lr=self.hparams.learning_rate\n",
    "        )\n",
    "        \n",
    "        # Learning rate scheduler\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=self.hparams.warmup_steps,\n",
    "            num_training_steps=self.trainer.estimated_stepping_batches\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'interval': 'step',\n",
    "                'frequency': 1\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "def train_mbti_classifier(\n",
    "    datamodule,\n",
    "    project_name: str = \"mbti-classification\",\n",
    "    experiment_name: Optional[str] = None,\n",
    "    max_epochs: int = 10,\n",
    "    gpus: int = 1,\n",
    "    **model_kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the MBTI classifier with comprehensive WandB logging.\n",
    "    \n",
    "    Args:\n",
    "        datamodule: MBTIDataModule instance\n",
    "        project_name: WandB project name\n",
    "        experiment_name: WandB run name (optional)\n",
    "        max_epochs: Maximum number of training epochs\n",
    "        gpus: Number of GPUs to use\n",
    "        **model_kwargs: Additional arguments for MBTIClassifier\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize WandB logger\n",
    "    wandb_logger = WandbLogger(\n",
    "        project=project_name,\n",
    "        name=experiment_name,\n",
    "        log_model=True  # Save model checkpoints to WandB\n",
    "    )\n",
    "    \n",
    "    # Log hyperparameters\n",
    "    wandb_logger.experiment.config.update({\n",
    "        \"batch_size\": datamodule.batch_size,\n",
    "        \"max_epochs\": max_epochs,\n",
    "        **model_kwargs\n",
    "    })\n",
    "    \n",
    "    # Initialize model\n",
    "    model = MBTIClassifier(\n",
    "        num_classes=datamodule.num_classes,\n",
    "        **model_kwargs\n",
    "    )\n",
    "    \n",
    "    # Callbacks\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor='val/f1',\n",
    "        mode='max',\n",
    "        dirpath='checkpoints/',\n",
    "        filename='mbti-{epoch:02d}-{val_f1:.3f}',\n",
    "        save_top_k=3,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    early_stop_callback = EarlyStopping(\n",
    "        monitor='val/f1',\n",
    "        patience=3,\n",
    "        mode='max',\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=max_epochs,\n",
    "        accelerator='gpu' if gpus > 0 else 'cpu',\n",
    "        devices=gpus if gpus > 0 else 'auto',\n",
    "        logger=wandb_logger,\n",
    "        callbacks=[checkpoint_callback, early_stop_callback, lr_monitor],\n",
    "        log_every_n_steps=10,\n",
    "        gradient_clip_val=1.0,\n",
    "        precision='16-mixed' if gpus > 0 else 32,  # Use mixed precision for faster training\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    trainer.fit(model, datamodule)\n",
    "    \n",
    "    # Test\n",
    "    trainer.test(model, datamodule)\n",
    "    \n",
    "    # Close WandB\n",
    "    wandb.finish()\n",
    "    \n",
    "    return model, trainer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009840d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Raw data found at: data/raw/mbti_1.csv\n",
      "INFO:__main__:Loaded 8675 rows\n",
      "INFO:__main__:Data split: Train=6246, Val=694, Test=1735\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The anonymous setting has no effect and will be removed in a future version.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: [wandb.login()] Loaded credentials for https://api.wandb.ai from /home/prg/.netrc.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpablorocg10\u001b[0m (\u001b[33mpablorg\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.24.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>wandb/run-20260116_100317-vq57i9uc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/pablorg/mbti-distilbert/runs/vq57i9uc' target=\"_blank\">distilbert-finetuning-v1</a></strong> to <a href='https://wandb.ai/pablorg/mbti-distilbert' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/pablorg/mbti-distilbert' target=\"_blank\">https://wandb.ai/pablorg/mbti-distilbert</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/pablorg/mbti-distilbert/runs/vq57i9uc' target=\"_blank\">https://wandb.ai/pablorg/mbti-distilbert/runs/vq57i9uc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3070 Ti Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "INFO:__main__:Raw data found at: data/raw/mbti_1.csv\n",
      "INFO:__main__:Loaded 8675 rows\n",
      "INFO:__main__:Data split: Train=6246, Val=694, Test=1735\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/home/prg/GitHub/mlops_group70/.venv/lib/python3.12/site-packages/pytorch_lightning/utilities/model_summary/model_summary.py:242: Precision 16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">    </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name           </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type                                </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Mode  </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> FLOPs </span>┃\n",
       "┡━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0  </span>│ model          │ DistilBertForSequenceClassification │ 67.0 M │ eval  │     0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1  </span>│ criterion      │ CrossEntropyLoss                    │      0 │ train │     0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 2  </span>│ train_acc      │ MulticlassAccuracy                  │      0 │ train │     0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3  </span>│ val_acc        │ MulticlassAccuracy                  │      0 │ train │     0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4  </span>│ test_acc       │ MulticlassAccuracy                  │      0 │ train │     0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5  </span>│ val_precision  │ MulticlassPrecision                 │      0 │ train │     0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 6  </span>│ val_recall     │ MulticlassRecall                    │      0 │ train │     0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 7  </span>│ val_f1         │ MulticlassF1Score                   │      0 │ train │     0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 8  </span>│ test_precision │ MulticlassPrecision                 │      0 │ train │     0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 9  </span>│ test_recall    │ MulticlassRecall                    │      0 │ train │     0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 10 </span>│ test_f1        │ MulticlassF1Score                   │      0 │ train │     0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 11 </span>│ test_confusion │ MulticlassConfusionMatrix           │      0 │ train │     0 │\n",
       "└────┴────────────────┴─────────────────────────────────────┴────────┴───────┴───────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━┳━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m  \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName          \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType                               \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMode \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mFLOPs\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━╇━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0 \u001b[0m\u001b[2m \u001b[0m│ model          │ DistilBertForSequenceClassification │ 67.0 M │ eval  │     0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1 \u001b[0m\u001b[2m \u001b[0m│ criterion      │ CrossEntropyLoss                    │      0 │ train │     0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m2 \u001b[0m\u001b[2m \u001b[0m│ train_acc      │ MulticlassAccuracy                  │      0 │ train │     0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m3 \u001b[0m\u001b[2m \u001b[0m│ val_acc        │ MulticlassAccuracy                  │      0 │ train │     0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m4 \u001b[0m\u001b[2m \u001b[0m│ test_acc       │ MulticlassAccuracy                  │      0 │ train │     0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m5 \u001b[0m\u001b[2m \u001b[0m│ val_precision  │ MulticlassPrecision                 │      0 │ train │     0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m6 \u001b[0m\u001b[2m \u001b[0m│ val_recall     │ MulticlassRecall                    │      0 │ train │     0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m7 \u001b[0m\u001b[2m \u001b[0m│ val_f1         │ MulticlassF1Score                   │      0 │ train │     0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m8 \u001b[0m\u001b[2m \u001b[0m│ test_precision │ MulticlassPrecision                 │      0 │ train │     0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m9 \u001b[0m\u001b[2m \u001b[0m│ test_recall    │ MulticlassRecall                    │      0 │ train │     0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m10\u001b[0m\u001b[2m \u001b[0m│ test_f1        │ MulticlassF1Score                   │      0 │ train │     0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m11\u001b[0m\u001b[2m \u001b[0m│ test_confusion │ MulticlassConfusionMatrix           │      0 │ train │     0 │\n",
       "└────┴────────────────┴─────────────────────────────────────┴────────┴───────┴───────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 67.0 M                                                                                           \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 67.0 M                                                                                               \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 267                                                                        \n",
       "<span style=\"font-weight: bold\">Modules in train mode</span>: 11                                                                                          \n",
       "<span style=\"font-weight: bold\">Modules in eval mode</span>: 96                                                                                           \n",
       "<span style=\"font-weight: bold\">Total FLOPs</span>: 0                                                                                                     \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 67.0 M                                                                                           \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 67.0 M                                                                                               \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 267                                                                        \n",
       "\u001b[1mModules in train mode\u001b[0m: 11                                                                                          \n",
       "\u001b[1mModules in eval mode\u001b[0m: 96                                                                                           \n",
       "\u001b[1mTotal FLOPs\u001b[0m: 0                                                                                                     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/prg/GitHub/mlops_group70/.venv/lib/python3.12/site-packages/rich/live.py:256: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/prg/GitHub/mlops_group70/.venv/lib/python3.12/site-packages/rich/live.py:256: UserWarning: install \n",
       "\"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/home/prg/GitHub/mlops_group70/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:534: Found 96\n",
       "module(s) in eval mode at the start of training. This may lead to unexpected behavior during training. If this is \n",
       "intentional, you can ignore this warning.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/home/prg/GitHub/mlops_group70/.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:534: Found 96\n",
       "module(s) in eval mode at the start of training. This may lead to unexpected behavior during training. If this is \n",
       "intentional, you can ignore this warning.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9928aa54a2c44a4bb35142dd10020ec3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     from mbti_datamodule import MBTIDataModule  # Import your DataModule\n",
    "    \n",
    "# Initialize DataModule\n",
    "dm = MBTIDataModule(\n",
    "    batch_size=16,  # Smaller batch size for transformer models\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "# Prepare data\n",
    "dm.prepare_data()\n",
    "dm.setup()\n",
    "\n",
    "# Train model with WandB logging\n",
    "model, trainer = train_mbti_classifier(\n",
    "    datamodule=dm,\n",
    "    project_name=\"mbti-distilbert\",\n",
    "    experiment_name=\"distilbert-finetuning-v1\",\n",
    "    max_epochs=10,\n",
    "    gpus=1,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=500,\n",
    "    max_length=512,\n",
    "    dropout_rate=0.1,\n",
    "    freeze_layers=0  # Fine-tune all layers\n",
    ")\n",
    "\n",
    "print(\"Training completed!\")\n",
    "print(f\"Best model checkpoint: {trainer.checkpoint_callback.best_model_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mbti-classifier (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
